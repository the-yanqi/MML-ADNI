{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd473d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ca9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "adni_bids = '/scratch/yx2105/shared/MLH/data/bids_part2/'\n",
    "adni_clinical_bids = '/scratch/yx2105/shared/MLH/data/clinical_bids_part2/'\n",
    "subject_sessions_list_file =  'subject_sessions_list_2.tsv'\n",
    "\n",
    "\n",
    "adni_bids = '/scratch/yx2105/shared/MLH/data/bids_part1/'\n",
    "adni_clinical_bids = '/scratch/yx2105/shared/MLH/data/bids_part1/'\n",
    "subject_sessions_list_file =  'subject_sessions_list_1.tsv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b01388",
   "metadata": {},
   "source": [
    "# 1. Prepare clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf59ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of columns that can be used from ADNI BIDS:\n",
    "participant_columns = ['alternative_id_1', 'participant_id',\"sex\", \"education_level\",\"marital_status\", \"apoe4\", \"apoe_gen1\", \"apoe_gen2\",\"diagnosis_sc\"]\n",
    "\n",
    "session_columns = [\"age\",\n",
    "                   # Cognitive measures\n",
    "                   \"MMSE\", \"cdr_sb\", \"cdr_global\", \"adas11\", \"adas13\",\n",
    "                   \"adas_memory\", \"adas_language\", \"adas_concentration\", \"adas_praxis\", \"ravlt_immediate\", \"moca\",\n",
    "                   \"TMT_A\", \"TMT_B\", \"dsst\", \"logmem_delay\", \"logmem_imm\",\n",
    "                   # RAVLT score\n",
    "                   \"neurobat_ravlt_forgetting\", \"neurobat_ravlt_learning\", \"neurobat_ravlt_perc_forgetting\",\n",
    "                   # everyday cognition test score\n",
    "                   \"ecogpt_ecogpttotal\", \"ecogsp_ecogpttotal\",\n",
    "                   # T1 measures\n",
    "                   \"adni_ventricles_vol\", \"adni_hippocampus_vol\", \"adni_brain_vol\", \"adni_entorhinal_vol\",\n",
    "                   \"adni_fusiform_vol\", \"adni_midtemp_vol\", \"adni_icv\",\n",
    "                   # PET measures\n",
    "                   \"adni_fdg\", \"adni_pib\", \"adni_av45\",\n",
    "                   # CSF measures\n",
    "                   \"adni_abeta\", \"adni_tau\", \"adni_ptau\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ab224",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_tsv = pd.read_csv(os.path.join(adni_bids, \"participants.tsv\"), sep='\\t')\n",
    "subj_sessions = pd.read_csv(os.path.join('/scratch/yx2105/shared/MLH/data/', subject_sessions_list_file) , sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d2b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_tsv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412618f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_series = {}\n",
    "session_series = {}\n",
    "for col in participant_columns:\n",
    "    participant_series[col] = []\n",
    "for col in session_columns:\n",
    "    session_series[col] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b23a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We collect the specified columns data\n",
    "for row in subj_sessions.iterrows():\n",
    "    subj_sess = row[1]\n",
    "    # From the participants.tsv file for each subject\n",
    "    selected_participant = participants_tsv[(participants_tsv.participant_id == subj_sess.participant_id)].iloc[0]\n",
    "    for col in participant_columns:\n",
    "        participant_series[col].append(selected_participant[col])\n",
    "        \n",
    "\n",
    "    # From the different sessions.tsv files for each subject and session\n",
    "    session_tsv = pd.read_csv(os.path.join(adni_clinical_bids, subj_sess.participant_id,\n",
    "                                        subj_sess.participant_id + \"_sessions.tsv\"), sep='\\t')\n",
    "    selected_session = session_tsv[(session_tsv.session_id == subj_sess.session_id)].iloc[0]\n",
    "    for col in session_columns:\n",
    "        if col in selected_session:\n",
    "            session_series[col].append(selected_session[col])\n",
    "        else:\n",
    "            session_series[col].append(np.nan)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55233a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add collected information to subjects .tsv\n",
    "for col in participant_columns:\n",
    "    subj_sessions.loc[:, col] = pd.Series(participant_series[col], index=subj_sessions.index)\n",
    "\n",
    "for col in session_columns:\n",
    "    subj_sessions.loc[:, col] = pd.Series(session_series[col], index=subj_sessions.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df79382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We replace gender information that is text by numeric values\n",
    "subj_sessions.loc[subj_sessions[subj_sessions.sex == 'F'].index, 'sex'] = 1\n",
    "subj_sessions.loc[subj_sessions[subj_sessions.sex == 'M'].index, 'sex'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda9e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_sessions.to_csv(os.path.join(adni_clinical_bids, 'all_clinical_data.tsv'), sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b90c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c67e3e6b",
   "metadata": {},
   "source": [
    "## Load data and create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163138e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_sessions_1 = pd.read_csv(os.path.join('/scratch/yx2105/shared/MLH/data/bids_part1', 'all_clinical_data.tsv'), sep='\\t')\n",
    "subj_sessions_1['data_dir'] = '/scratch/yx2105/shared/MLH/data/bids_part1'\n",
    "subj_sessions_2 = pd.read_csv(os.path.join('/scratch/yx2105/shared/MLH/data/clinical_bids_part2/', 'all_clinical_data.tsv'), sep='\\t')\n",
    "subj_sessions_2['data_dir'] = '/scratch/yx2105/shared/MLH/data/bids_part2'\n",
    "subj_sessions = pd.concat([subj_sessions_1, subj_sessions_2], ignore_index=True, sort=False)\n",
    "subj_sessions = subj_sessions.sort_values(['participant_id','session_id']).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e55bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_sessions#.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99df449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get next session time and label\n",
    "\n",
    "img_dirs = []\n",
    "diagnosis_12months = []\n",
    "for id, row in subj_sessions.iterrows():\n",
    "    participant_id = row['participant_id']\n",
    "    session_id = row['session_id']\n",
    "    session_id_12 = 'ses-M{}'.format(int(session_id.split('M')[-1]) + 12)\n",
    "    row_12 = subj_sessions[(subj_sessions['participant_id'] == participant_id) & (subj_sessions['session_id'] == session_id_12)]\n",
    "    if len(row_12) == 0:\n",
    "        diagnosis_12months.append('')\n",
    "    else:\n",
    "        diagnosis_12months.append(row_12.iloc[0]['diagnosis_sc'])\n",
    "\n",
    "    file_name = '{}_{}_{}'.format(participant_id,session_id,'T1w.nii.gz')\n",
    "    img_dir = os.path.join(row['data_dir'],participant_id,session_id,'anat',file_name)\n",
    "    exist = os.path.exists(img_dir)\n",
    "    if exist:\n",
    "        img_dirs.append(img_dir)\n",
    "    else:\n",
    "        img_dirs.append('')\n",
    "\n",
    "subj_sessions['diagnosis_12month'] = diagnosis_12months\n",
    "subj_sessions['img_dir'] = img_dirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_sessions_12month = subj_sessions[(subj_sessions['diagnosis_12month'] != '') &(subj_sessions['diagnosis_12month'] != 'SMC')]\n",
    "subj_sessions_12month = subj_sessions_12month[(subj_sessions['img_dir'] != '')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_sessions_12month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7822e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_12month = subj_sessions_12month[~subj_sessions.sex.isnull() &\n",
    "                        ~subj_sessions.education_level.isnull() &\n",
    "                        ~subj_sessions.apoe4.isnull() &\n",
    "                        ~subj_sessions.MMSE.isnull() &\n",
    "                        ~subj_sessions.cdr_sb.isnull() &\n",
    "                        ~subj_sessions.adas_memory.isnull() &\n",
    "                        ~subj_sessions.adas_language.isnull() &\n",
    "                        ~subj_sessions.adas_concentration.isnull() &\n",
    "                        ~subj_sessions.adas_praxis.isnull() &\n",
    "                        ~subj_sessions.ravlt_immediate.isnull()].reset_index()\n",
    "\n",
    "model_all = subj_sessions[~subj_sessions.sex.isnull() &\n",
    "                        ~subj_sessions.education_level.isnull() &\n",
    "                        ~subj_sessions.apoe4.isnull() &\n",
    "                        ~subj_sessions.MMSE.isnull() &\n",
    "                        ~subj_sessions.cdr_sb.isnull() &\n",
    "                        ~subj_sessions.adas_memory.isnull() &\n",
    "                        ~subj_sessions.adas_language.isnull() &\n",
    "                        ~subj_sessions.adas_concentration.isnull() &\n",
    "                        ~subj_sessions.adas_praxis.isnull() &\n",
    "                        ~subj_sessions.ravlt_immediate.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e2b74",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389d9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = subj_sessions_12month\n",
    "\n",
    "# remove columns that has more than 70% missing values\n",
    "new_cols = []\n",
    "for col in all_df.columns:\n",
    "    if (all_df.isnull().sum()/len(all_df) < 0.7)[col]:\n",
    "        new_cols.append(col)\n",
    "    \n",
    "all_df = all_df[new_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6e26db",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7902ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_missing = all_df.isnull().sum()/len(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb99a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = []\n",
    "for col in all_df:\n",
    "    if len(all_df[col].unique()) < 25:\n",
    "        print(col,len(all_df[col].unique()),all_df[col].unique())\n",
    "        cat_col.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_missing[cat_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c59b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = all_df\n",
    "\n",
    "# impute for categorical variables\n",
    "clean_df.loc[:,'apoe4'] = clean_df['apoe4'].fillna(clean_df['apoe4'].mode().iloc[0])\n",
    "clean_df.loc[:,'cdr_global'] = clean_df['cdr_global'].fillna(clean_df['cdr_global'].mode().iloc[0])\n",
    "\n",
    "clean_df.loc[:,['apoe_gen1','apoe_gen2','adas_concentration']] = clean_df[['apoe_gen1','apoe_gen2','adas_concentration']].fillna(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd30ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad19aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute for numerical variable\n",
    "num_cols_small = ['age','MMSE', 'cdr_sb','adas11','adas13', 'ravlt_immediate']\n",
    "clean_df[num_cols_small] = clean_df[num_cols_small].fillna(clean_df[num_cols_small].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f90a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get df with missing values\n",
    "missing_df = clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0724d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the size of the dataset\n",
    "# clean_df = clean_df[~clean_df.logmem_delay.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b2ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop numerical cols with too large missing values\n",
    "clean_df = clean_df.drop(['adas_memory','adas_language', 'adas_praxis', 'dsst','adni_fdg'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a4dd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with relatively large missing values with mean\n",
    "num_cols_large = ['moca','logmem_delay', 'logmem_imm','adni_ventricles_vol','adni_hippocampus_vol', 'adni_brain_vol','adni_entorhinal_vol','adni_fusiform_vol',\n",
    "'adni_midtemp_vol', 'adni_icv']\n",
    "\n",
    "clean_df[num_cols_large] = clean_df[num_cols_large].fillna(clean_df[num_cols_large].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb830f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_df = clean_df.drop(num_cols_large,axis = 1)\n",
    "clean_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35ab788",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84901bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = clean_df\n",
    "cat_col = [\n",
    " 'marital_status',\n",
    " 'apoe4',\n",
    " 'apoe_gen1',\n",
    " 'apoe_gen2',\n",
    " 'cdr_global',\n",
    " 'adas_concentration']\n",
    "\n",
    "dummy_df = pd.get_dummies(processed_df[cat_col].astype('category'))\n",
    "\n",
    "processed_df = processed_df.drop(cat_col,axis=1)\n",
    "processed_df = pd.concat([processed_df,dummy_df],axis=1,join = 'inner').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49309c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e2c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffef758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# split data into train and test sets based on the \"id\" column\n",
    "X = processed_df.drop(\"diagnosis_12month\", axis=1)\n",
    "y = processed_df[\"diagnosis_12month\"]\n",
    "gss = GroupShuffleSplit(n_splits=100, test_size=0.2)\n",
    "\n",
    "for train_index, test_index in gss.split(X, y, groups=processed_df[\"participant_id\"]):\n",
    "    X_train_all, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_all, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "X_train_all = X_train_all.reset_index(drop=True)\n",
    "y_train_all = y_train_all.reset_index(drop=True)\n",
    "print(len(X_train_all))\n",
    "\n",
    "gss2 = GroupShuffleSplit(n_splits=100, test_size=0.2)\n",
    "for train_index, val_index in gss2.split(X_train_all, y_train_all, groups=X_train_all[\"participant_id\"]):\n",
    "    X_train, X_val = X_train_all.iloc[train_index], X_train_all.iloc[val_index]\n",
    "    y_train, y_val = y_train_all.iloc[train_index], y_train_all.iloc[val_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33090edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train,y_train],axis=1).reset_index(drop=True)\n",
    "val = pd.concat([X_val,y_val],axis=1).reset_index(drop=True)\n",
    "test = pd.concat([X_test,y_test],axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83801f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list = ['participant_id','session_id','alternative_id_1','diagnosis_sc','diagnosis_12month', 'data_dir','img_dir']\n",
    "tab_columns = [i for i in train.columns if i not in remove_list]\n",
    "\n",
    "X_train = train.loc[:,tab_columns]\n",
    "X_val = val.loc[:,tab_columns]\n",
    "X_test = test.loc[:,tab_columns]\n",
    "\n",
    "# create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler to the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform the training and test sets using the scaler\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "train.loc[:,tab_columns] = X_train_scaled\n",
    "val.loc[:,tab_columns] = X_val_scaled\n",
    "test.loc[:,tab_columns] = X_test_scaled\n",
    "\n",
    "# train.to_csv('/scratch/yx2105/shared/MLH/data/train_large.csv')\n",
    "# val.to_csv('/scratch/yx2105/shared/MLH/data/val_large.csv')\n",
    "# test.to_csv('/scratch/yx2105/shared/MLH/data/test_large.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed7708f",
   "metadata": {},
   "source": [
    "## Load processed data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95adb66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/scratch/yx2105/shared/MLH/data/train.csv',header=0,index_col=0)\n",
    "val = pd.read_csv('/scratch/yx2105/shared/MLH/data/val.csv',header=0,index_col=0)\n",
    "test = pd.read_csv('/scratch/yx2105/shared/MLH/data/test.csv',header=0,index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aa2fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a03f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "train = pd.read_csv('/scratch/yx2105/shared/MLH/data/train.csv',header=0,index_col=0)\n",
    "val = pd.read_csv('/scratch/yx2105/shared/MLH/data/val.csv',header=0,index_col=0)\n",
    "test = pd.read_csv('/scratch/yx2105/shared/MLH/data/test.csv',header=0,index_col=0)\n",
    "\n",
    "mapping = {\n",
    "    'CN': 0,\n",
    "    'AD': 1,\n",
    "    'LMCI':2,\n",
    "    'EMCI':2,\n",
    "    'MCI':2,\n",
    "    'SMC':2 \n",
    "}\n",
    "\n",
    "train['diagnosis_12month'] = train['diagnosis_12month'].astype(\"category\").map(mapping)\n",
    "val['diagnosis_12month'] = val['diagnosis_12month'].astype(\"category\").map(mapping)\n",
    "test['diagnosis_12month'] = test['diagnosis_12month'].astype(\"category\").map(mapping)\n",
    "\n",
    "\n",
    "remove_list = ['participant_id','session_id','alternative_id_1','diagnosis_sc','diagnosis_12month', 'data_dir','img_dir']\n",
    "tab_columns = [i for i in train.columns if i not in remove_list]\n",
    "\n",
    "X_train = train.loc[:,tab_columns]\n",
    "X_val = val.loc[:,tab_columns]\n",
    "X_test = test.loc[:,tab_columns]\n",
    "\n",
    "y_train = train.loc[:,'diagnosis_12month']\n",
    "y_val = val.loc[:,'diagnosis_12month']\n",
    "y_test = test.loc[:,'diagnosis_12month']\n",
    "\n",
    "print((len(test[test['diagnosis_12month']==0])))\n",
    "print((len(test[test['diagnosis_12month']==1])))\n",
    "print((len(test[test['diagnosis_12month']==2])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7f79dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccaf59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X_train = pd.concat([X_train,X_val],axis=0)\n",
    "all_y_train = pd.concat([y_train,y_val],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c571a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# create a OneVsRestClassifier object with a logistic regression model\n",
    "classifier = OneVsRestClassifier(LogisticRegression(C=0.01))\n",
    "\n",
    "# perform 5-fold cross-validation and evaluate with the AUROC score\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "\n",
    "auroc_scores = cross_val_score(classifier, all_X_train, all_y_train,cv=cv, scoring=\"roc_auc_ovr\")\n",
    "f1_fivefold = cross_val_score(classifier, all_X_train, all_y_train, cv=cv, scoring=\"f1_macro\")\n",
    "acc_fivefold = cross_val_score(classifier, all_X_train, all_y_train, cv=cv, scoring=\"balanced_accuracy\")\n",
    "\n",
    "# classifier.fit(X_train, y_train)\n",
    "# predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5116141",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_fivefold.mean(),auroc_scores.mean(),f1_fivefold.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad5885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)\n",
    "predictions_probs = classifier.predict_proba(X_test)\n",
    "predictions = np.argmax(predictions_probs,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test,predictions_probs,multi_class=\"ovr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32bc45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test,predictions,average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49583a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc14eb",
   "metadata": {},
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ad872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# create an XGBClassifier with the \"multi:softmax\" objective and 3 classes\n",
    "xgd_classifier = XGBClassifier(objective=\"multi:softmax\", num_class=3, learning_rate=0.1, max_depth=15)\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "auroc_fivefold = cross_val_score(xgd_classifier, all_X_train, all_y_train, cv=cv, scoring=\"roc_auc_ovr\")\n",
    "f1_fivefold = cross_val_score(xgd_classifier, all_X_train, all_y_train, cv=cv, scoring=\"f1_macro\")\n",
    "acc_fivefold = cross_val_score(xgd_classifier, all_X_train, all_y_train, cv=cv, scoring=\"balanced_accuracy\")\n",
    "\n",
    "print('acc:', acc_fivefold.mean())\n",
    "print('auroc_ovr:', auroc_fivefold.mean())\n",
    "print('f1_macro:', f1_fivefold.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af50bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgd = xgd_classifier.fit(all_X_train, all_y_train)\n",
    "xgd_classifier.feature_importances_\n",
    "\n",
    "predictions_probs = xgd_classifier.predict_proba(X_test)\n",
    "predictions = np.argmax(predictions_probs,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff6d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test,predictions_probs,multi_class=\"ovr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15881dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test,predictions,average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e8b1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fe12d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgd_feature_importance = pd.DataFrame(list(xgd.get_booster().get_fscore().items()), \\\n",
    "                                      columns = ['feature','importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "\n",
    "from xgboost import plot_importance\n",
    "plt.figure(figsize=(18,20))\n",
    "plot_importance(xgd)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f129a417",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"Makes the main denoising auto\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_shape [int] : input shape\n",
    "    enc_shape [int] : desired encoded shape\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_shape, out_cls, enc_shape = 8):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Linear(in_shape, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, enc_shape),\n",
    "        )\n",
    "        \n",
    "        self.decode = nn.Sequential(\n",
    "            nn.BatchNorm1d(enc_shape),\n",
    "            nn.Linear(enc_shape, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, in_shape)\n",
    "        )\n",
    "        self.linear = nn.Linear(in_shape, out_cls)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        x = self.decode(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c08810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to test the model with the test dataset and print the accuracy for the test images\n",
    "def testAccuracy(X_test, y_test, model):\n",
    "    \n",
    "    model.eval()\n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(X_test)\n",
    "        y_pred = nn.functional.softmax(output, dim = 1)\n",
    "        \n",
    "        _, y_pred_class = y_pred.max(dim=1)\n",
    "        #print(y_pred_class)#.shape,y_test.shape)\n",
    "        auroc = roc_auc_score(y_test,y_pred,multi_class=\"ovr\")\n",
    "        f1 = f1_score(y_test,y_pred_class, average = 'macro')\n",
    "        b_acc = balanced_accuracy_score(y_test,y_pred_class)\n",
    "    return auroc, f1, b_acc\n",
    "\n",
    "def train(model, error, optimizer, n_epochs, X, y, X_test, y_test):\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = error(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            \n",
    "            auroc = testAccuracy(X_test, y_test, model)\n",
    "            print(f'epoch {epoch} \\t Test Loss: {loss.item():.4g} \\t Test AUC: {auroc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d02b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder = Autoencoder(in_shape=X_train.shape[1], out_cls=3, enc_shape=2).double()\n",
    "\n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(auto_encoder.parameters(), lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a3ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(np.array(X_train))\n",
    "y = torch.from_numpy(np.array(y_train))\n",
    "\n",
    "X_t = torch.from_numpy(np.array(X_test))\n",
    "y_t = torch.from_numpy(np.array(y_test))\n",
    "train(auto_encoder, error, optimizer, 400,X,y, X_t, y_t )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4815b8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1fd42e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.load('/scratch/yx2105/shared/MLH/results/baseline_cnn_lr1_0/predictions_best_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f3e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc = roc_auc_score(y_val,preds,multi_class=\"ovr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b74bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e161530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
